\documentclass{article} 
\newcommand\tab[1][0.5cm]{\hspace*{#1}}

\title{COMPSYS304 Notes 2017} 
\author{Theodore Oswandi} 

\usepackage[
	lmargin=2.5cm,
	rmargin=2.5cm,
	tmargin=1cm,
	bmargin=3cm,
	]{geometry}
\usepackage{enumitem}
\setlist{noitemsep}
\usepackage[none]{hyphenat}
\usepackage{listings}
\setlength{\parindent}{0pt}

\begin{document} \maketitle{} 

\section{Lecture 1 \& 2}
	\subsection*{Improvements}
		\begin{itemize}
			\item Semiconductor technology and computer architecture improved lots
			\item Performance measured using standardised benchmarks
			\item Clock rate/frequency has also increased considerably in this time.
		\end{itemize}

	\subsection*{Computer Architecture}
		\textbf{\tab ISA:} Boundary between hardware and software\\
		\textbf{\tab Oragnisation:} high level computer design aspects\\
		\textbf{\tab Hardware:} detailed logic and circuit design
		\\ \\
		\textbf{\tab Note:} You want to separate your instruction set from implementation

	\subsection*{Memory Organisation}
		\begin{itemize}
			\item See memory as indexed 1D array
			\item Memory Access Time: time to read data to/from memory \\ 
			\item Memory Speed != Processor speed.
			\item Fast memory is very expensive. Heirarchy used to maintain fluid functionality and keep things cheap.
		\end{itemize}

		\paragraph{\tab Processor Registers}
		\begin{itemize}
			\item Smallest and fastest memory for CPU
			\item about 32-64 of them. Each 32/64bits in size.
			\item Nanosecond access time
		\end{itemize}

		\paragraph{\tab Cache Memory}
		\begin{itemize}
			\item Slower than register, but larger (8-256k)
			\item Few nanoseconds access time
			\item Levels (L1, L2, L3) used in multiprocessor systems.
		\end{itemize}

		\paragraph{\tab Main Memory}
		\begin{itemize}
			\item Slower than cache. But really big.
			\item Tens of nanoseconds lookup time.
		\end{itemize}

	\subsection*{Instruction Set Architecture (ISA)}
		\paragraph{\tab ISA} is interface between hardware and low level software. (80x86, MIPS, ARMs)

		\subsubsection*{Using Fixed ISAs}
			\paragraph{} Uses old instruction set (1970s), also used with extensions to enable newer technologies such as internet, etc...

			\paragraph{\tab Advantages}
			\begin{itemize}
				\item Can have different implementation of same architecture
				\item AMD/Intel both have same ISA but different implementation.
			\end{itemize}

			\paragraph{\tab Disadvantages}
				\begin{itemize}
					\item power consumption is higher than things like iPad which use different ISA and consume a lot less power
					\item Also prevent some new innovation since it is so widely used in today's world.
				\end{itemize}


		\subsubsection*{ISA Design}
			\paragraph{\tab Need to define:}
			\begin{itemize}
				\item Instruction Format and Encoding
				\item Data types and their sizes
				\item Location of operands and where to store results
			\end{itemize}

		\paragraph{\tab Operands and Opcodes} To carry out these calculations, an \textbf{opcode} must be defined to define these calculations. Upon these opcodes, zero to three \textbf{operands} are used for data inputs and result outputs.

	\subsection*{Architecture Types}
		\subsubsection*{Stack Base Architecture}
			\begin{itemize}
				\item Top of stack will contain result of operation.
				\item If receive ADD then processor knows next 2 inputs contain 2 numbers that need to be added.
				\item PUSH add something to top of stack.
				\item POP use value in top of stack.
				\item JVM designed to use Stack based architecture.
				\item ADD function has no operators. Operates on last 2 loaded values.
			\end{itemize}

		\subsubsection*{Accumulator Based Archictecture.}
			\begin{itemize}
				\item Using inputs from memory.
				\item Not used anymore today. Used in 1970s
				\item ADD function takes one operator, $mem_address$ which contains the value to add to above loaded value.
			\end{itemize}

		\subsubsection*{Register Memory Architecture}
			\begin{itemize}
				\item Currently used today as x86
				\item Uses register for input as well as access values from memory.
				\item ADD function contains 3 operator.
					\begin{enumerate}
						\item \textbf{Rd} Destination Register
						\item \textbf{Rs} Source Register
						\item \textbf{mem\_address} Address of value to add from memory
					\end{enumerate}
			\end{itemize}

		\subsubsection*{Register-Register Architecture}
			\begin{itemize}
				\item Operands from register.
				\item LOAD and STORE only way to access memory.
				\item Need to specify destination register for output.
				\item ADD function has 3 operators.
					\begin{enumerate}
						\item \textbf{Rd} Destination Register
						\item \textbf{Rs} Source Register 
						\item \textbf{Rt} Register containing other value you want to add
					\end{enumerate}
			\end{itemize}

		\subsubsection*{Examples}
			Example is $A(1000) + B(2000) = C(3000)$ in the 4 types of architectures
			
			\paragraph{\tab Stack Based Architecture\\}
					PUSH 1000 \\ \tab
					PUSH 2000 \\ \tab
					ADD \\ \tab
					POP 3000

			\paragraph{\tab Accumulator Based\\}
					LOAD 1000 \\ \tab
					ADD 2000 \\ \tab
					STORE 3000

			\paragraph{\tab Register Memory\\}
					LOAD R2, 1000 \\ \tab
					ADD R1, R2, 2000 \\ \tab
					STORE R1, 3000

			\paragraph{\tab Register Register\\}
					LOAD R2, 1000 \\ \tab
					LOAD R3, 2000 \\ \tab
					ADD R1, R2, R3 \\ \tab
					STORE R1, 3000


	\subsection*{ISA Classes}
		\paragraph{Classification} generally based on: Instruction word size, number of different instructions, and number of clock cycles to complete a given instruction.

		\subsubsection*{Classes}
			\paragraph{RISC} (Reduced Instruction Set Computers) all instruction words same size. Simpler decoding hardware. 
			\\ \tab \textbf{MIPS} is an example processor that uses this type of ISA.

			\paragraph{CISC} (Complex Instruction Set Computers) instruction word sizes may vary.Code footprint may be smaller than RISC due to condensing multiple RISC instructions into one CISC. 
			\\ \tab \textbf{Intel x86} is an example of processors based off this.

			\paragraph{EPIC} (Explicitly Parallel Instruction Computers) have parallel operations in their instruction set. The compiler is very important.
			\\ \tab \textbf{Intel Itanium} uses this kind of ISA.

		\subsubsection*{Abstractions}
			\paragraph{Abstractions} remove unnecessary details and hide complexity so that it is easier to understand. 

			\paragraph{\tab Instruction Processing in CPU}
			\begin{enumerate}
				\item \textbf{Fetching} access memory, get next instruction.
				\item \textbf{Decoding} Interprets the instruction. (Operation and data requried from memory/registers)
				\item \textbf{Execution} Perform operation. Uses processor and writes result to register/memory.
			\end{enumerate}

		\subsubsection*{Questions to ask when designing ISA}
			\begin{enumerate}
				\item What type of ISA should be used?
				\item What operations are needed?
				\item How data (operands) are provided in instructions?
				\item Instruction and Data word sizes?
			\end{enumerate}

	\subsection*{Extras}
		\paragraph{\tab Types of operations}
		\begin{itemize}
			\item \textbf{Arithmetic} Addition, Subtraction, Multiplication, Division
			\item \textbf{Logical} AND, OR, Lshift, Rshift
			\item \textbf{Memory Access} LOAD, STORE
			\item \textbf{Control Transfer} Conditional/Unconditional Branches
			\item \textbf{Special Purpose} will talk later
		\end{itemize}	

		\paragraph{Notes: Shifting} You have to be careful when shifting as if you're dealing with signed integers then you may be messing with the sign bit when trying to multiply/divide	

\section{Lecture 3 \& 4}
	\subsection*{ALU Operations}
		\begin{itemize}
			\item Add immediate uses value, not pointer to register
			\item No Subimmediate as if Addimmediate allows negatives.
			\item Destination register generally before source register/s
			\item Register 0 is \texttt{static final} containing all 0s, and cannot be written to
		\end{itemize}

		\paragraph{Endian-ness}
		\begin{enumerate}
			\item \textbf{Little Endian} Least significant bit at top of memory addresses \\
			LSB at addr, MSB at addr+3
			\item \textbf{Big Endian} Most significant bit at the top of memory addresses \\
			LSB at addr+3, MSB at addr
		\end{enumerate}

		\paragraph{Sizes of things} in relation to memory size \\ \tab Word=4bytes \\ 
		\tab halfword=2bytes \\ 
		\tab 1byte=byte\\ \\
		This course will use MIPS simlulator on PC called SPIM

		\paragraph{Memory addressing in MIPS} machine is C(r$_x$)
			\\ \tab Where C is contstant which may be used to reserve part of memory. 
			\\ \tab And Rx is the contents of a given register

		\begin{itemize}
			\item lw, sw = Load/Store word
			\item lh, sh = Load/Store half-word
			\item lb, sb = Load/Store byte
			\item \textbf{NOTE: } 	lbu = Load byte unsigned. No need to sbu as it will only store the relevant least significant byte in register
		\end{itemize}


	\subsection*{Class exercise}
		addi \$10, \$0, 0x3000 \\
		ori \$12, \$0, 0x8015 \\
		sw \$12, 512(\$10) \\ \\
		\$10 = 0000 3000 \\
		\$12 = 0000 8015 \\
		sw register to put \$10 is 512$_{10}$ + 8015$_{16}$ = 0x00003200 \\\\
		Therefore Big Endian: \tab 00 00 80 15 \\
		And Little Endian: \tab \tab 15 80 00 00

	\subsection*{Instruction Encoding Cont.}
		\begin{itemize}
			\item \textbf{Note: } For efficient instruction encoding, we classify different instructions and formats for faster decode.
			\item If 32 registers then need 5bits to encode pointer to relevant register.
			\item Opcode needs 6bits to be represented. Can encode 64 opcodes.
			\item Also need some bits to represent immediate values and offsets.
			\item Program Counter (PC) used to signify where execution has got up to and therefore next instruction to execute.
			
		\end{itemize}

		\subsubsection*{R-Type Format}
			\begin{itemize}
				\item 6bit OpCode \tab \tab \tab Operation Instruction
				\item 5bit Register \tab \tab \tab First register operand
				\item 5bit Register \tab \tab \tab Second register operand
				\item 5bit Register \tab \tab \tab Third register operand
				\item 5bit Shift \tab \tab \tab \tab Amount for shift instructions
				\item 6bit Function Code \tab Operation variant.
			\end{itemize}


		\subsubsection*{I-Type Format}
			\begin{itemize}
				\item 6bit OpCode
				\item 5bit Destination Register
				\item 5bit Source Register
				\item 16bit Offset/Immediate Value (Depending on instruction)
			\end{itemize}

		\subsubsection*{Jumping Memory Addresses.}
			\paragraph{Changing sequence of execution: } is done through use of \textbf{branch} and \textbf{jump} instructions. This is done to let you have if/else and loops.
			\begin{enumerate}
				\item[\textbf{Jump}] Jumps to location in memory (unconditionally) to get next instruction. Like a GOTO. Uses J-Type Format
				\item[\textbf{Branch}] lets you conditionally go to another point in memory, only if the condition is met. Uses I-Type Format
			\end{enumerate}

			\paragraph{\tab Jumping} [6bit OpCode][26bit TargetAddress]\\ Target address must be 32 bit, so to get this 26 bit value to 32 bit you shift the 26 bit number left 2 times, then add the 4 MSB of PC to front of value. This results in final expected 32bit address needed for the jump.

			\paragraph{\tab Branching} [6bit OpCode][5bit Reg1][5bit Reg2][16bit Label]
			\begin{itemize}
				\item Once again, the target address must be 32bit, so have to calculate it using enocded information in instruction.
				\item Conditional Branching is \textbf{PC-relative} meaning that the PC provides current address and the Label provides an offset.
			\end{itemize}

			\begin{itemize}
				\item \textbf{BEQ} branch equal, taking 2 inputs and a label if true
				\item \textbf{BNE} branch not equal, taking 2 inputs and a label if true
			\end{itemize}

	bne and beq only have 16 extra bits. Therefore need to get to 32bit target address. Need to use PC again. Use 14 bits after shifting the 16bits in bne/beq to left by 2.

	\paragraph{Another example}
	target address = 0100 0400 + 4  + 400 = 0100 0804 \\
	+ 4 from PC+4 \\
	+400 from 0x100 * 4 [left shift 2] \\
	\textbf{NOTE} Have to also consider is little/big Endian by looking at the machine code of original instruction. The 16bit offset at the end will let you know.

	\begin{lstlisting}
addi 	$11, $0, 100	 //Initialise counter as 100
lw 	$8, 0($10)	// Load word from R10 into R8
sra	$9, $8, 3	// Shift right arithmetic on that and save in R9
sw 	$9, 0($10)	//Store contents of R9 back into R10
addu 	$10, $10, 4	//find the address of next element
addi 	$11, $11, -1	//Decrement loop counter
bne 	$11, $0, L1
	\end{lstlisting}

\section{Lecture 5 \& 6}
	\subsection*{Example of while loop}
	\begin{lstlisting}
  Example: Translate the below code to MIPS
	while(w[i] == x)
		i = i + j
	\end{lstlisting}

	Given that i = \$3, j = \$4, x = \$5, w(int array) = ]\$6

	Answer:
	\begin{lstlisting}
loop: #perform loop test
	sll	$10, $3, 2	#Shift left 2 == x4
	add $10, $6, $10	#Get address of w[i]
	lw	$11, 0($10)	#$11 = w[i]
	#perform w[i] == x
	bne $11, $5, exit	#Exit when w[i] != x
	# i = i + j
	add $3, $3, $4
	j loop

exit:	#out of the for loop
	\end{lstlisting}

	\subsection*{Comparison Instructions}
	\begin{itemize}
		\item Used for things like (x $<$ y)
		\item Will set destination register different value depending on result of comparison
		\item Will then use bne/beq to check if lessthan or greater than.
	\end{itemize}

	Examples:
	\begin{itemize}
		\item \textbf{slt} Rd, Rs, Rt = signed less than
		\item \textbf{sltu} Rd, Rs, Rt = unsigned less than
		\item \textbf{slti} Rd, Rs, Immed  = signed immediate less than
		\item \textbf{sltiu} Rd, Rs, Immed  = unsigned immediate less than
	\end{itemize}

	\subsection*{Load Upper Immediate}
		No implicit instruction to copy from one register to another. But can use contents of \$0 with the ADD or OR function to achieve the same effect. Use a pseudo instruction (scriptlike) called \textbf{Load Upper Immediate (lui)}

		Used when loading values to upper section of a register. Operations like \texttt{ori} and \texttt{andi} do this for the bottom half of a register

	\subsection*{Integer Multiplication and Division}
		Since multiplying two 32bit numbers generates a 64bit number, need to store it somewhere.

		\texttt{LO} and \texttt{HI} registers are special registers that hold the result of the multiplication or division.

		\begin{itemize}
			\item \textbf{mflo \$REG} move from \texttt{LO}
			\item \textbf{mfhi \$REG} move from \texttt{HI}
		\end{itemize}

	\subsection*{Subroutines}
		Procedure that gets called multiple times. Used to create modular program.
		\subsubsection*{Issues:}
		\begin{itemize}
			\item Need to call it.
			\item Need to give it arguments sometimes.
			\item Need it to return values sometimes.
		\end{itemize}

		\subsubsection*{Unconditional Jumps:}
		\begin{itemize}
			\item \textbf{j address} Jump to the 26bit address doing the shifting and 4MSB of PC attached.
			\item \textbf{jal address} (Jump and link) lets you jump to subroutine. Also uses \$31 used to store return address (PC+4 saved to the register)
			\item \textbf{jr register} Uses the value of register number as target address. Return instruction.
		\end{itemize}

		\subsubsection*{Conditional Branches:}
		\begin{itemize}
			\item \textbf{beq r1, r2, offset} 
			\item \textbf{bne r1, r2, offset} \\
			offset being 16 bit number (then shifted 2bits left and added to PC+4)
		\end{itemize}

		\subsubsection*{Relevant Registers:}
		\begin{itemize}
			\item \textbf{\$a0, \$a1, \$a2, \$a3} used for integer arguments
			\item \textbf{\$v0} used as return register
			\item \$sp is stack pointer register
			\item \$fp is frame pointer register
		\end{itemize}

		Stack frames size should be multiple of 16.

		\subsubsection*{Memory Layout:}
		\begin{itemize}
			\item Stack Segment (goes down) \tab 0x7fffffff
			\item Data Segment (goes up)
			\item Text Segment \tab[2.5cm] 0x40000000
			\item Reserved Memory
			\begin{itemize}
				\item Used for special OS tasks
			\end{itemize}
		\end{itemize}

		If have nested functions then need to save and restore value of \$31. Save this in the current stack so the value can be extracted and restored.\\

		Send 5th and onwards function input arguments using the stack space.
		

\section{Lectures 7 \& 8}
	\subsection*{Stack Frame Usage Summary}
		\begin{itemize}
			\item Procedure that doesn't call another one is a \textbf{leaf}
			\item Leaf procedure doesn't save \$ra
			\item Store values in registers used by calling procedure.
			\item Most of time local variables use registers, but can be stack frame sometimes.
		\end{itemize}

	\subsection*{MIPS assembly programming}
		\begin{itemize}
			\item Main CPU with ALU and special registers for mult/div
			\item Coprocessor for floating point stuff
			\item Coprocessor for traps and memory
		\end{itemize}

		\subsubsection*{Assembly directives}
		Identifiers are used to provide commands to assembler, prefaced with a dot (.)\\

	\textbf{Rest of the lecture about SPIM}

\newpage
\section{Lecture 9 \& 10}
	\subsection*{Jump Tables}
		More efficient way to do switch-case statements than to do a bunch of if-else statements.

		\begin{lstlisting}
switch(k){
	case 0:		f = i + j;	break;
	case 1:		f = g + h;	break;
	case 2:		f = g - h;	break;
	case 3:		f = i - j;	break;
}

JumpTable  .word	L0, L1, L2, L3	#Creating jump table

	jr k		#Jump to relevant k value from jump table

L0:	add f, i, j	#k = 0, f = i + j
	j exit

L1:	add f, g, h	#k = 1, f = g + h
	j exit

L2:	add f, g, -h	#k = 2, f = g - h
	j exit

L3: add f, i, -j	#k = 3, f = i - j. No break;

exit:
		\end{lstlisting}

	\subsection*{Floating Point Representation}
		\subsubsection*{Single Precision}
		\begin{itemize}
			\item bit 31 \tab \tab Sign bit
			\item bit 30-23 \tab Exponent
			\item bit 22-00 \tab Mantissa
		\end{itemize}

		\subsubsection*{Double Precision}
		\begin{itemize}
			\item bit 63 \tab \tab Sign bit
			\item bit 62-52 \tab Exponent
			\item bit 51-00 \tab Mantissa
		\end{itemize}
		k
		\subsubsection*{Extras}
		\begin{itemize}
			\item \textbf{zero} \tab \tab Exponent and Mantissa are all 0s
			\item \textbf{Infinity} \tab Exponent 255, Mantissa 0s. Sign determines if positive or negative.
			\item \textbf{NaN} \tab \tab Exponent 255, non-zero mantissa
		\end{itemize}

	\subsection*{MIPS}
		MIPS has a floating point co-processor with special registers (pairs for doubles) to represent floats.\\

		Co-processor itself has 32 32bit registers. \$f0 \$f31.

		\subsubsection*{Registers}
		\begin{itemize}
			\item 00 - 02 \tab Results of a function
			\item 04 - 10 \tab Temp registers
			\item 12 - 14 \tab Pass first 2 arguments
			\item 16 - 18 \tab Temp registers
			\item 20 - 30 \tab Caller saved registers
		\end{itemize}

		\textbf{lwc1} and \textbf{swc1} are used to load/store values to the coprocessor 1 (floats)

		\subsubsection*{Operations}
		\begin{itemize}
			\item \textbf{Single} \tab add.s, sub.s, mul.s, div.s
			\item \textbf{Double} \tab add.d, sub.d, mul.d, div.d
		\end{itemize}

	\subsection*{Compare Instructions}
		You use \$31 to see comparison instructions between 2 floats.\\

		Function \texttt{c.X.s} or \texttt{c.X.d} is used to compare the values in the registers and return value saved in \$31. X can represent:
		\begin{itemize}
			\item eq = equal
			\item neq = not equal
			\item lt = less than
			\item le = less than or equal
			\item gt = greater than
			\item ge = greater than or equal
		\end{itemize}

	\subsection*{Converting}
		Can convert from any Integer/Word, Single Float, Double Float to any other using \texttt{cvt.X.Y} where y is [w = word, s = single, d = double]

	\subsection*{Moving to/from coprocessor}
	\textbf{mtcZ/mfcZ} is move to/from coprocessor. Z value = 1 represents co-processor for floating point.\\

	mtc1 \$12, \$15 moves value from Register12 to Register15 in coprocessor1.\\

	This function just moves the bits to/from the registers and doesn't do the conversion.

\section{Lecture 11 \& 12}
	Digital Circuit are
	\begin{itemize}
		\item \textbf{Combinational}	output only dependent on current input value
		\item \textbf{Sequential} 	output dependent on current input and state
	\end{itemize}

	\subsection*{Propagation Delay}
		\begin{itemize}
			\item It exists
			\item Charging and discharging of load capacitance means nonzero D$_{prop}$
			\item Need to take this into account or may get glitches
		\end{itemize}

	\subsection*{Sequential Circuits}
		\begin{itemize}
			\item Have storage element to keep state
			\item Flipflops have delays, and are edge triggered.
			\item D$_{setup}$ minimum time for input before change in CLK
			\item D$_{hold}$ time output needs to be stable after CLK edge.

		\end{itemize}

	\subsection*{Implementing}
		Area between ISA and Hardware. Organisation of aspects of computer design.

	\subsection*{CPU implementation}
		\begin{itemize}
			\item \textbf{Control Unit} Generate signals to direct datapath
			\item \textbf{Datapath} perform CPU operations\\

			\item \textbf{Control Signals} from CU to DP
			\item \textbf{Status Signals} from DP to CU
		\end{itemize}

	\subsection*{Datapath components}
		\subsubsection*{Arithmetic Logic Unit - ALU}
			\begin{itemize}
				\item Arithmetic + Local operations on register value.
				\item Has list of operations it can do
				\item Extra output for \texttt{Zero} or \texttt{Overflow} result
				\item 3bit code to specify what operation you want to do.
			\end{itemize}

		\subsubsection*{Register File}
			\begin{itemize}
				\item Provide operands for operations
				\item Has bunch of potential inputs.
				\item Get values of input registers, used to represent what registers need to be written/read to.
			\end{itemize}

		\subsubsection*{Program Counter - PC}
			\begin{itemize}
				\item Indicate address of next instruction
				\item Signals for reading/writing
			\end{itemize}

		\subsubsection*{Memory Interfacing}
			\begin{itemize}
				\item Used to access memory for instructions
			\end{itemize}

		\subsubsection*{Sign Extension}
			\begin{itemize}
				\item Change 16bit signed number to 32bit signed number
			\end{itemize}

	\subsection*{ISA Subset Implementation}
		\subsubsection*{Single Cycle Implementation}
			\begin{itemize}
				\item Assume single clock cycle for each instruction.
				\item Longer clock cycle to accommodate this.
				\item \textbf{Duty Cycle} ratio of system being active to being inactive.
			\end{itemize}

		Fetch $\rightarrow$ Decode $\rightarrow$ Execute (and loop back around)

			\begin{itemize}
				\item \textbf{Fetch} Read instruction word from memory, increment PC.
				\item \textbf{Decode} Use fields from instruction to get values necessary and ignore un-needed registers
			\end{itemize}

	\subsection*{Datapath for Section}
		\subsubsection*{Fetch}
			\begin{itemize}
				\item PC - to get address of next instruction
				\item Register File - put instruction address
				\item ALU - for incrementing PC
			\end{itemize}

		\subsubsection*{R-Type Instruction}
			\begin{itemize}
				\item Register File - get value from register
				\item ALU - Perform operation on value of register
				\item Register File - specify output register.
			\end{itemize}

		\subsubsection*{Load/Store}
			\begin{itemize}
				\item ALU - result of ALU to Memory Interfacing
				\item Register File - Desination register.
			\end{itemize}

	\subsection*{Immediate values and ALU}
		Can get value straight from register (not through register file). Ensure it goes through Sign Extender first, to convert 16bit input to 32bit input.

	\subsection*{Implementing BEQ Example}
		\begin{itemize}
			\item Use ALU and subtract
			\item Check if \texttt{zero} bit is set in ALU (will be set if values are the same)
			\item Need to set new address if \texttt{zero} value is set
		\end{itemize}

		\begin{itemize}
			\item ALU ADD - used for calculating new memory address
			\item ALU SUB - used to check for equality
			\item Control Signal for ALU
			\begin{itemize}
				\item lw 00
				\item sw 00
				\item beq 01
			\end{itemize}
		\end{itemize}
		
\section{Lecture 13 \& 14}
	Single Cycle Datapath Impelementation cont.
		(11) not correct as inputs for read/write registers for src/dest of some operatiions not guaranteed. Some have OP rs rd and some have OP rd rs

		(12) Sequential circuits need to have clock inputs, not shown in the diagram.

		(14) have another section used for generating and allocating the various control signals
		- easiest way to set these control signals is to just generate a table to map 1/0 values to the various outputs needed for each opcode

		Control Unit
			Input = 6bit Opcode
			Output = various control signals for sections of datapath

			Outputs:
				RegDst
				ALUsrc
				MemtoReg
				RegWrite
				MemRead
				MemWrite
				branch
				ALUop1
				ALUop0

			This control section is combinational since it just takes the opcode and assert values on the output control signals without need of clock

		Implementing Jump Instruction
			Jump insturctions doesn't use registers
			Since it changes PC unconditionally, only datapath section that relates to PC needs to be extended. 
			Need to create new control signal to gate either jump or branch instruction

		(20) can't execute archive instructions on given datapath 

		Implement BNE Instruction
			Can still use the sub with ALU and zero bit
			Also can use the register loading from BEQ
			Need to create new control signal to distinguish BNE and BEQ
				Same as BEQ, but you negate the zero value
			Then OR the result  of both the BEQ and BNE AND gates

		Implementing addi
			Can modify ALU control
			Or can also choose not to modify ALU at all and change how the registers are loaded, however this is harder to do


		Example, list values of control signals for:   
			ori \$10, \$11, 0x101A

			RegDst		0	Not reading from third register
			Jump		0	Not jump
			Branch\_ne	0	Not Branch
			Branch\_eq	0	Not Branch
			Memread		0	Don't read memory
			MemtoReg	0	Using value of address
			ALUop		11	Use value for immediate value (no table given)
			Memwrite	0
			ALUsrc		1
			Regwrite	1

	Improving Performance
		In reality these components have delays associated with them
		Memory Unit = 2ns
		ALU \& Adder = 2ns
		Register File (R/W) = 1ns
		Others = assumed to be negligible

		6ns for archive instruction (R-type)
			2ns for getting from memory
			1ns for reading from register
			2ns for ALU adding
			1ns for writing to register

		LW instruction
			DIDN"T GET IT

		Example 2
			Timing
				SInce single cycle implementation then get the highest delay as the total delay per cycle. No need to add all delays

		Performance Single Cycle Implementation
			Comparing:
			\begin{enumerate}
				\item single-cycle impelmentation with pessimistic clock timing due to longest instruction
				\item Single cycle implementation where each clock cycle only runs for as long as it needs to. Not possible in real life, but just for conceptual purposes
			\end{enumerate}

			IC = Instruction Count

		Multicycle implementation
		\begin{itemize}
			\item Uses multiple clock cycles of clock that runs faster than doing everything on one long clock cycle
			\item Different clock cycles used for each of the different phases of execution of an instruction
			\item control units have to become more complex
			\item however datapath may become more simpler
		\end{itemize}

		\begin{itemize}
			\item has shared memory unit and ALU
			\item don't need to separate instruction and memory fetch since can be done in multiple cycles like in single cycle
			\item Single memory interface enough for this implementation
		\end{itemize}

			Goal: to separate it to allow for multiple cycles for a single instruction

\section{Lecture 15 \& 16}
	Multicycle implementation of MIPS ISA
		\begin{itemize}
			\item Mutltiple clock cycle for single full instruction
			\item Some functional units not used in some clock cycles, can share between instructions
			\item Need to store state between these cycles. So will need some temporary registers
			\item Simpler data path, but more complex control unit as a result
		\end{itemize}

	High elvel view
		ALUout are temporary register to keep value of ALU computed in a clock cycle
		IR write, Memory Data, RegisterReadA, RegisterReadB, and ALUout are temporary registers
		
		Problem - to figure out how many clock cycles to allocate to each of the different types of instructions.

		3 different sections 
		\begin{enumerate}
			\item Get from memory section
			\item Register Section
			\item ALU section
		\end{enumerate}

		Some 4 clock cycle instructions
		\begin{enumerate}
			\item Get from memory
			\item Load registers
			\item Calculate result
			\item Store result back to memory
		\end{enumerate}

		R type
		\begin{itemize}
			\item Instruction Fetch
			\item Decode/Register Fetch
			\item Execution 
			\item Memory Access, finish R-type, write ALUout to register values
		\end{itemize}

		Memory Reference
		\begin{itemize}
			\item Instruction Fetch
			\item Decode/Register Fetch
			\item Execution
			\item Memory Access, either LOAD or STORE
			\begin{itemize}
				\item If load, then need to finish memory read
			\end{itemize}
		\end{itemize}

		Branch Instructions
		\begin{itemize}
			\item Instruction Fetch
			\item Decode/Register Fetch
			\item Execution, and update PC to ALUout
		\end{itemize}

		Jump Instructions
		\begin{itemize}
			\item Instruction Fetch
			\item Decode/Register Fetch
			\item Execution, and update PC to register values
		\end{itemize}

		IorD = used to signify when you want to use PC as location of memory address when set to 0

		For a given set of signals that need to be asserted, make sure that other signals won't affect the intended result of the asserted signals

		TODO
		Explanation of Control Unit Signals
		\begin{itemize}
			\item Outputs Control
			\begin{enumerate}
				\item PCWriteCond
				\item PCWrite
				\item IorD
				\item MemRead
				\item MemWrite
				\item MemtoReg
				\item IRWrite
				\item PCSource
				\item ALUOp
				\item ALUSrcB
				\item ALUSrcA
				\item RegWrite
				\item RegDst
			\end{enumerate}
		\end{itemize}
		
	Also need to calculate average cycles per instruction

	Loading takes highest number of cycles, therefore reducing this should make overall performance faster if doing multiple cycle datapath

	Exaple with machines with different characteristics
		Look at overall FSM and combine states that are in description of machines, as they are performed on the same clock cycle
		Then look at critical path of number of cycles and determine the machine with best overall performance 

	Performance depends on both clock period and average instructions per instruction so need to find best balance between the two.

\section{Lecture 17 \& 18}
	Went over question on slide 4.

	Exceptions and Interrupts
		MIPS uses interrupt for expected event, and exception for unexpected event
		When happens, control transferred to Exception Handler/Interrupt Service Routine that processes event that occured.
		Then return back to program to resume.

		Causes
			External Event
				Timer, Keyboard/Mouse input - All asynchronous things that we can't predict all the time.

			Internal Events
				Traps, happens inside the code for a situation like divide by 0, or overfllow
				Software Interrupts, some instructions that create an interrupt.
				These things are synchronous, happen at a certain point in the code.

				Fixed by handler
				Simulated if do not have necessary hardware to do something (Something old did not have float\_coprocessor, so would emulate it)
				Set result to null

	MIPS convention for exceptions
		SOMETABLE


	Identifying what caused an exception
		EPC
		CAUSE
		BADVADDR
		STATUS

		MIPS uses single address to store the exception handling code due to memory constraints.
		Is at 0x8000 0180, and in kernel somewhere.

		If using spim, use eret

		Classic user mode and kernel mode (3 levels of privilege for kernel mode)

	Status Register
		register 12, coprocessor 0
		bit0	Interrupt enable (1 = enabled)
		bit1	Exception level (1 = exception has occured)
		bits8-15	Interrupt mask (6 hardware interrupt, 2 software interrupt)

		Register 13, co0
		bit 2-6	Encoding cause for interrupt/excp
		bit 8-15	Pending interrupt mask

	Some more examples
		Overflow
			ADD, ADDI, SUB will cause exception if overflow
			ADDU will not, will just get wrong result if that is the case

		Address Error

		System Call

		MIPS EXAMPLE

	Adding Exception Handling to multicycle datapath
		Only consider
			undefined instruction
			arithmetic overflow

		Special registers EPC and Cause used
			The value of cause will determine which kind of exception was thrown

\section{Lecture 17 \& 18}
	Pipelining
		Basically doing stuff concurrently if not dependent on each other
		If washing machine done, put in drier so next person can use washing machine

		Execution Latency; DEFNITION
		Throughput, total work done in given time

	Multi-cycle to Pipeline
		Switching them is difficult as some resources are required in every clock cycles for some kinds of instructions
		All instructions should have same execution latency to make pipelining chunks of work easier.

		If have $M$ pipelined stages and $N$ instructions, then need $M + N - 1$

		Therefore cycles per instruction approaches 1
		$CPI = \frac{M + N - 1}{N}$
		Assume ideal case when doing calculation ($CPI = 1$)

	Pipeline Datapath
		Need to have pipeline registers to keep information between the clock cycles for the specific instructions

		Register if read, RHS is shaded
		If written then LHS is shaded

		In single cycle, use clock cycles to assert control signals
		In multi-cycle then use the FSM to assert clock signals

		Control signals also pipelined for the following stage, so registers also contain state for next stage signals
		Put it between Instruction Fetch and Instruction decode, to allocate for Execution and Mem/WB

	Pipelining Hazards
		Not everything in the world is ideal
		Due to nature of pipelining, if doing operations that depend on each other. If not correctly spaced apart, may read old values and operate on them instead of new value

		These are caled hazards
		They reduce performance if not handled correctly
		May need to stall pipeline to guarantee the values are read and written correctly and as expected.


		Structural Hazards
			Resource conflict
			Simultaneous access of same resource

		Data Hazards
			Instruction dependent on result of another, need to wait for that one to be done first

		Control Hazards
			Instructions exist in pipeline that need PC to be changed.

		Detect and Resolution
			\textbf{Forwarding Mechanism}- Pipe data straight from result of ALU into register input.

			Forwarding
				Check if rt/rs of given instruction is same as rd of previous instruction.
				Also need to check if not same as 2 previous destination. To prevent the value being piped 2 instructions ahead when 3 consecutive instrustions depend on value of single register.

			Stalling
				If next instruction dependent on value of previous instruction that hasn't been generated yet. 
				Need to stall for $n$ clock cycles to let it generate the value expected. 
				Done by adding delay cycles to stall overall execution of instructions.
				Check if memread (loading word), and then if previous pipeline register is expecting same value, then need to delay for a cycle.

				If need to do this, make sure that PC isn't changed as it may skip the executing of the instruction you want to delay
				Therefore make operation have no effect by setting all control signals to 0 in EX, MEM, and WB stages






\section{Extra Morteza}
	\subsection{Single Cycle Implementation Control Signals}
		\begin{enumerate}
			\item \textbf{RegDst}
				\\Denotes if ALU will be used
				\begin{itemize}
					\item \textbf{ZERO} if ALU not used. For things like \texttt{mult, div, jr, mfc0}
					\item \textbf{ONE} if ALU used. For thing slike \texttt{add, sub, or, and, lt}
				\end{itemize}

			\item \textbf{Branch}
				\\Denotes if branching will happen. Feeds into an AND gate.
				\begin{itemize}
					\item If \textbf{ONE} then will get 16 bit offset, shift left by 2, and add 4MSB of PC+4
					\item If \textbf{ZERO} then continue using PC+4 as next instruction
				\end{itemize}

			\item \textbf{MemRead}
				\\Only used for LoadWord operations, however can be used for some R-type

			\item \textbf{MemtoReg}
				\\Only used for LoadWord operations

			\item \textbf{ALUop}
				\\Denotes what operation the ALU will do
				\begin{itemize}
					\item 000 \tab AND
					\item 001 \tab OR
					\item 010 \tab ADD
					\item 011 \tab SUB
					\item 111 \tab LessThan
				\end{itemize}

			\item \textbf{MemWrite}
				\\Only used for StoreWords operation

			\item \textbf{ALUsrc}
				\\Used for Load and Store opeations. Denotes if ALU uses register or 16bit immediate value from instruction

			\item \textbf{RegWrite}
				\\R-type and load word use. Potentially used for write data for Register File.
		\end{enumerate}

	\subsection{Multiple Cycle Implementation Control Signals}
		\begin{enumerate}
			\item \textbf{RegDst}
				\\Denotes if ALU will be used
				\begin{itemize}
					\item \textbf{ZERO} if ALU not used. For things like \texttt{mult, div, jr, mfc0}
					\item \textbf{ONE} if ALU used. For thing slike \texttt{add, sub, or, and, lt}
				\end{itemize}

			\item \textbf{Branch}
				\\Denotes if branching will happen. Feeds into an AND gate.
				\begin{itemize}
					\item If \textbf{ONE} then will get 16 bit offset, shift left by 2, and add 4MSB of PC+4
					\item If \textbf{ZERO} then continue using PC+4 as next instruction
				\end{itemize}

			\item \textbf{MemRead}
				\\Only used for LoadWord operations, however can be used for some R-type

			\item \textbf{MemtoReg}
				\\Only used for LoadWord operations

			\item \textbf{ALUop}
				\\Denotes what operation the ALU will do
				\begin{itemize}
					\item 000 \tab AND
					\item 001 \tab OR
					\item 010 \tab ADD
					\item 011 \tab SUB
					\item 111 \tab LessThan
				\end{itemize}

			\item \textbf{MemWrite}
				\\Only used for StoreWords operation

			\item \textbf{ALUsrc}
				\\Used for Load and Store opeations. Denotes if ALU uses register or 16bit immediate value from instruction

			\item \textbf{RegWrite}
				\\R-type and load word use. Potentially used for write data for Register File.
		\end{enumerate}



				

\section{Caches 1 - Generic/Introduction}
	\subsection*{CPUs need fast memory}
		CPUs have frequencies of 3GHz, and \textbf{random} memory accessing takes approximately 1.2ns (and occurs about 25\% of the time). The fact that random is bolded is due to the fact that sequential accessing is much much faster than random access.\\

		Problem is that \textbf{RAM} has 40ns+ access time. Therefore faster cache exist and if had to only rely on that, the CPU would be doing nothing and waiting a lot of the time.\\

		However, \textbf{Static RAM} (SRAM) is much faster at 0.5-2ns access times, but is very expensive. Also need to be \textbf{small to be fast.} The larger the total size, the larger it is and the longer it takes for you to access something at a random location.\\

		Classic memory heirarchy again of [Cache, RAM, Disk]

	\subsection*{Cache Principle}
		Keep memory consistent and fast.
		\begin{itemize}
			\item Higher levels contain copies of the lower level caches
			\item If try get memory location, start at lower level and go up.
		\end{itemize}

		\subsubsection*{Cache}
		\begin{itemize}
			\item Transparent to program
			\item Hold subset of memory
			\item Commonly accessed data/instr in cache
			\item Different levels in itself. (Usually 3, but can be 2)
		\end{itemize}

	\subsection*{Locality of Reference}
		You don't want to just store random things in the cache. Want to actually have a plan of what you want to put there.

		<<SOMETHING>>

		\paragraph{Spatial Locality}
		If you access an item at a given space, you are likely to access another item next to it, or closeby in memory. Therefore loading more than one item at a time may produce speedup. 

		\paragraph{Temporal Locality}
		If you access an item, you are likely to do it again within a certain timeframe. Can be things like looping through a section of memory.

	\subsection*{Cache Operation}
		Ask cache first. If there then is a HIT and return it. Otherwise ask memory (or level of cache).

		\paragraph{Hit rate}
		Times you hit, per number of memory accesses. The higher the rate the better it is. Different memory access times per level. Measured by timing when you first try get the resource, up until you get it.

		\paragraph{Miss rate}
		Times you don't get a hit for avery 

		\paragraph{Miss penalty}
		Access time after miss.

	\subsection*{Cache benefit}
		<<SOMETHING>>

		The higher you go the faster the memory is perceived.

	\subsection*{Example}
		L1 = 4 clock	5\%miss
		L2 = 11 clock	2.5\% gloal miss
		L3 = 21 clock	1.5\% global miss
		RAM = 120clock

		Therefore

		(4 * 0.95) + (11 * 0.025) + (21 * 0.01) + (120 * 0.015) = 6.085 cycles of perceived memory access time

\section{Caches 2 - Organisation}
	\subsection*{Cache Design}
		\subsubsection*{Objectives}
		\begin{itemize}
			\item High hit rate
			\item Low access time\\
			Can't do much about access time in this course, as it is reliant on underlying implementation of hardware)
		\end{itemize}

		\subsubsection*{Points to consider}
		\begin{itemize}
			\item Size of cache
			\item Block/Line size (doesn't handle bytes or words, but larger entities called blocks and lines)
			\item Data location mapping
			\item Replacement (cache is small, gotta remove to add)
			\item Write policy (what happens if processor wants to write to memory location, and keep consistency)
			\item Levels of cache, exlusive/inclusive
		\end{itemize}

	\subsection*{Direct Mapping}
		Directly using the bits of the address to where it is in memory.

		Look at lower 3 bits and see where it is mapped in memory. But repeated many times in main memory so there is competition for all things with same suffix to have that spot in cache.

		If it goes into cache, it goes into into the same position in cache.


	<<STUFF>>
		\begin{itemize}
			\item \textbf{Valid bit}. Associated with cache entry. Represents if that position in cache has value loaded in it
			\item \textbf{Index}. Lower part of address, specifies where it will go if cached
			\item \textbf{Tag}. Upper part of address, specifies memory location
		\end{itemize}

		Therefore [Tag][Index] represents full memory address of relevant cached resource.

	Address to cache entry
		[Tag][index][Offset]

		<<YAY>>

	Cache Metrics
		Machine dependent
			q = word size
			p = address size

		Byte Addressed Memory
			m = byte offset bits (dependent on word size)
			b = block bits
			k = index bits

		64byte line example
			18bit tag
			8bit index
			4bit block offset
			2bit byte offset

	Direct Mapping Analysis
		Simple :) 
			both conceptually and hardware

		Low hardware cost :)
			single comparator

		High potential for conflict :(
			<<XD>>


Fully Associative
	Memory location go to any line of cache
	Line index no relation to address
		Larger TAG
		no K
		no line index
	Comparator for every line
		Lots of comparators
		Memory location could be anywhere
		Sequential comparison slow

	Replacement policy needed.

	<<MORE YAY>>

Set Associative
	Hybrid
		Doesn't have a single direct mapped section, but multiple of them.

	Depends how wide or tall you want to go with the design.

N - way associative
	Performance simnilar to fully associative, as n increases.
	Much less hardware than having it fully associative

Performance Comparison
	Using SPEC2000
	15Kbyte cache, 64bytes per line
	p = 32bit word, 16 per line
	k = 8 = 256 lines

	Associativity
	1	10.3\%
	2	8.5\%
	4	8.3\%
	8	8.1\%

	If change size of cache, then significant redution in miss rate up to approx 64KB cache size. Negligible after then. Most significant from 1KB to 2KB

	<<STOPPED LISTENING>> @ slide 18

\section{Cache 3}
	Line replacement
	Replacement Policy
		Optimal Policy
			If you used it recently, you might want to use it again
			remove the item that is oldest (has oldest last access time)

		Random
			Doesn't trigger worst case scenario
			Bad for average performance

		FIFO
			Replace longest item that's been in cache
			Round robin over sets
			Needs k pointers.
			Doesn't mean that something isn't used if in cache for a long time.

		LRU (Least Recently Used)
			Replace line that was accessed long ago
			Normally not good enough
			Can be difficult to implement

			Can be done by "flipping" the bits so that next cache position changes every time.

			4 way pretty good but an approximation
			8 way is a single more level
			16 way gets to the point where it isn't worth it anymore to continue getting deeper as the cost of storing all these bits increases.
				8 line pairs
				Choose random pair and choose single LRU

	Writing to memory
		Due to nature of multilevel cache, if you write to something you have to ensure that it propagates throughout all levels of cache.
		Don't want file written to to be replaced and updates lost.

		Write through
			Every write goes through memory. Write updates go through heirarchy

			If cache hit
				modify cache line
				write to memory

			If cache miss
				Write allocate
					Item put into cache
					Write over it
					Then update propagates through levels to memory

				Write no allocate
					CPU ignores cache and updates memory directly.

			Uses write buffer so that processor doesn't stall.
			Writes done in parallel with other proceses, and has low priority compared to reads.

		Write Back
			Write only goes to cache
			Modification indicated with \textbf{dirty bit}

			If cache hit
				modify cache line
				set bit

			If cache miss
				Evict conflicting cache line
					Write to memory if dirty bit set
				Fetch cache line
				modify cache line
				set dirty bit
			Can also use a write buffer for cache misses


		Comparison
			Write Through
				Efficient Use of bus interface queuing
					Priority to reads
				Lots of traffic
					Cannot use burst mode
						Only one word written, not entire line

			Write Back
				Better for typical program. When generated writes > bandwidth
				More difficult to implement.

\section{Cache 4}
	\subsection{Optimising cache performance}
		\subsubsection{Performance determined by}
			\begin{enumerate}
				\item Miss rate
				\item Hit rate
				\item Miss penalty
			\end{enumerate}

		\subsubsection{Causes of cache misses}
			\begin{itemize}
				\item Compulsory misses
				\begin{itemize}
					\item First time cache line accessed
					\item Reduced by larger blocks
				\end{itemize}

				\item Capacity Misses
				\begin{itemize}
					\item Working set > Cache size
					\item Reduced by larger cache
				\end{itemize}

				\item Conflict misses
				\begin{itemize}
					\item Data compete for same cacheline
					\item Reduced by higher associativity (\& Better replacement policy)
					\item Reduced by larger cache
					\item Reduced by \textbf{smaller} block
					\begin{itemize}
						\item The larger the cache line, the more misses you get (more conflicts)
					\end{itemize}
				\end{itemize}
			\end{itemize}

		\subsubsection{Changing Cache Aspects}
			\begin{enumerate}
				\item Larger cache size
				\begin{itemize}
					\item :) \tab Reduce miss rate
					\item :( \tab Increases hit time
				\end{itemize}

				\item Larger cache blocks
				\begin{itemize}
					\item :) \tab reduces miss rate (compulsory)
					\item :( \tab Increases miss rate (conflict)
					\item :( \tab Increases miss penalty (mitigated with higher bandwidth)
				\end{itemize}

				\item Higher associativity
				\begin{itemize}
					\item :) \tab Reduces miss rate
					\item :( \tab Increases hit time (possibly)
				\end{itemize}

				\item Write through/back
				\begin{itemize}
					\item No clear influence on cache performance
					\item More influence on system bus traffic
				\end{itemize}
			\end{enumerate}

	\subsection{Multilevel Cache}
		Solution to memory not being able to be fast and large at the same time
		\begin{enumerate}
			\item L1 cache
			\begin{itemize}
				\item Small
				\item Split cache (DATA and INSTRUCTION)
				\item Hit time very important
			\end{itemize}

			\item L2 \& L3 Cache
			\begin{itemize}
				\item Larger (lines may also be larger)
				\item Unified (not split to I and D)
				\item Reduces miss penalty for L1
				\item Faster than main, slower than L1
			\end{itemize}
		\end{enumerate}

		\begin{itemize}
			\item \textbf{Inclusive}
			\begin{itemize}
				\item Populated from main memory up to L1
				\item Simple management
				\item Coherent easily (if in L2, also in L3)
				\item Total size is size of L3
				\item Used by Intel
			\end{itemize}

			\item \textbf{Exclusive}
			\begin{itemize}
				\item If exist in L2, doesn't exist in other cache levels
				\item Complex management
				\item Difficult to keep coherent
				\item Works like victim cache
				\begin{itemize}
					\item Line removed from L2 goes to L3
				\end{itemize}
				\item Total size is sum of size of levels of cache (higher than inclusive)
				\item Used by AMD Bulldozer/Piledriver
			\end{itemize}
		\end{itemize}

	\subsection{Programming for cache performance}
		\subsubsection*{Objectives}
			\begin{itemize}
				\item Minimise miss rate
				\item Hide latency
			\end{itemize}

		\begin{itemize}
			\item \textbf{Programmers}\tab can't change hardware, access times or penalties
			\item \textbf{Prefetching}\tab hides latency by loading cache line before accessing it. HW = CPU prefetching, SW = machine instruction
			\item \textbf{Bypassing}\tab reduces miss rate by not caching writes. SW = machine instructions
		\end{itemize}

		\subsubsection{Mimising miss rate}
			\begin{itemize}
				\item Programmer needs to know size of cache, line size and organisation.
				\item Try to use as much of lines as possible, physically and frequency before overwriting it.
				\item Maximise spatial and temporal locality
			\end{itemize}

		\subsubsection{Programming approaches: adjust...}
			\begin{itemize}
				\item Size of working set
				\begin{itemize}
					\item Divide problems to smaller chuck to fit in cache level
					\item Usually not use full cache size, due to competition with instructions and OS calls.
				\end{itemize}

				\item Access patterns
				\begin{itemize}
					\item Algorithm have freedom
					\item Choose best for performance (row major and column major)
				\end{itemize}

				\item Stride
				\begin{itemize}
					\item Longer the stride the worse it is.
				\end{itemize}
			\end{itemize}

\section{Virtual Memory 1}

\section{Virtual Memory 2}
	\subsection{TLB - Translation Look-aside Buffer}
		\subsubsection{Performance problem}
			Have to translate every address before it is accessed
			Therefore 3 memory accesses required: page table, data and address + translation
			More chance for cache miss.

			TLB in memory management unit of every modern CPU.
			Address translation done before going to L1 cache.

		\subsubsection{TLB metrics}
			\begin{itemize}
				\item Metrics - Typical
					\begin{itemize}
						\item Size: 16-512 entries
						\item Block size: 1-2 page table entries
						\item Hit time: 0.5-1 cycle
						\item Miss penalty: 10-100 cycles
						\item Miss rate: 0.01\% - 1\%
						\item Levels: 1-2
						\item Split: data and instruction split
					\end{itemize}
					
				\item Associativity
					\begin{itemize}
						\item TLB’s are small
						\item Need to be fast
						\item Fully associative OK for small caches
						\item LRU too expensive
						\item Rarely low associativity
						\item Miss expensive, aim for low miss rate
					\end{itemize}
			\end{itemize}

		(6) the addresses of physical page number + page offset are the same as address tag + cache index + block offset + byte offset

		amd has level 0 caches as their fastest level
		\subsubsection{TLB misses}
		\begin{itemize}
			\item Not the same as page fault!
			
			\item Page hit
			\begin{itemize}
				\item Can be handled by hardware or software
			\end{itemize}
			
			\item Page fault - When quick translation doesn't find anything and therefore not in memory and must get from disk
			\begin{itemize}
				\item Handled by software
			\end{itemize}
			
			\item Software is invoked via exceptions
		\end{itemize}

\section{Parallelisation 1}
	\subsection{Principles}
		Motivation = 
		Types =

		\subsubsection{Forms of CPU parallelism}
		\begin{itemize}
			\item Instruction level
			\begin{itemize}
				\item Pipeline
				\item Multiple issue
				\begin{itemize}
					\item Large instruction word (static)
					\item Superscalar (dynamic)
				\end{itemize}
				\item SIMD
			\end{itemize}
			\item Thread level parallelism
			\begin{itemize}
				\item Simultaneous multithreading
				\item Modules - AMD bulldozer
				\item Multicore
			\end{itemize}
		\end{itemize}

	\subsection{Thread level parallelism}
		Have to rewrite the programs
		Frequency of processors aren't improving much anymore, therefore need to utilise more cores.

		SMT - Simultaneous multithreading (Hyperthreading)
			Still single pipeline, but 2 registers for each pipeline stage
			Can process 2 threads
			Superscalar (usually)
			Situations where HT is good, and others where it is bad/neutral

		Modules - AMD bulldozer
			halfway between SMT and multicore
			Almost everything doubled, except for IF, ID and commit unit

		Multicore
			Multiple independent processor on the same die
			Share caches (partially)
			Combined with SMT/HT

\section{Parallelism 2}
	\subsection{Memory and parallel systems}
		Location and access
			Location
				Centralised in PC
				Distributed in med-large parallel systems

			Access
				Shared memory with multicore, pc, servers
				Message passing in clusters and large parallel systems

			Focus on shared memory and multicore processors
			u






	\subsection{Shared memory multiprocessors}



































			



\end{document}
